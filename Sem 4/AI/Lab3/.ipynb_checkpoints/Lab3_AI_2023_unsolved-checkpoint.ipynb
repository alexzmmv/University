{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.I. Assignment 3\n",
    "\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "* Perform some more data preproscessing: checking for missing samples, eliminate them, encoding labeled classes\n",
    "* Feel comfortable creating a simple decision tree for a classification \n",
    "* Dealing with some feature selection techniques\n",
    "\n",
    "### Content:\n",
    "\n",
    "The Lab. has 3 sections: \n",
    "\n",
    "1. Preprocessing\n",
    "2. Constructing and fitting a decision tree\n",
    "3. Perform a feature selection based on the linear correlation between the dataset's features \n",
    "\n",
    "There are some exercises, put your answer code in the cells with the *# your code here*. \n",
    "\n",
    "All the work must be done during the lab and uploaded on teams by the end of the lab. \n",
    "\n",
    "If there are any python libraries missing, please install them on your working environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a variation of the famous **Iris dataset**. !DO NOT USE OTHER VERSION THAN THE ONE PROVIDED HERE.  \n",
    "\n",
    "This set contains measurements of various parts of three different species of iris flowers. The goal is to use these measurements to predict the species of an iris flower.\n",
    "\n",
    "The dataset contains 154 instances, each with 4 features: *sepal length*, *sepal width*, *petal length*, and *petal width*. The target variable is the species of the iris flower, which can be one of three possible values: *setosa*, *versicolor*, or *virginica*.\n",
    "\n",
    "We will start by loading the dataset, preparing it and splitting it into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# the imports:\n",
    "\n",
    "# pandas for handling the data\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn is a Python data visualization library that offers a user-friendly interface \n",
    "#    for generating visually appealing and informative statistical graphics.\n",
    "import seaborn as sns\n",
    "\n",
    "# From sklearn we import some classes and functions for data handling, the tree classifier, \n",
    "#    the accuracy and the plot function to depict the tree  \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.tree import plot_tree \n",
    "\n",
    "# This class we use it to search exhaustive over specified parameter values for an estimator.\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "print(\"hei\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Import with pandas the file *iris_teach_2.csv* into the pandas DataFrame with the name *df_iris*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. use the method *isnull()* from the class DataFrame to check if there are empty cells in the dataset. (Hint: check the documentation and use this method with respect to your DataFrame object; use the method .sum() to the result to count the empty cells on columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We see that we have some empty cells on some rows. Delete these rows (hint: use the method *dropna()* from pandas.DataFrame class, with the argument *inplace=True*). Check the documentation why we use that argument (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Divide the dataset in two parts: a set **X** for features and **y** for target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a **LabelEncoder** object to encode the classes from the target. Fit it with the *y* list, and encode *y* with it. (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Divide the dataset in a training and a testing set as we did it in the previous laboratory with the sklearn function *train_test_split*. Check the documentation why we use for *random_state* a fixed value here! (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "We can now build the decision tree model using scikit-learn's **DecisionTreeClassifier class**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tree_clf \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m) \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Fit the classifier to the training data \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tree_clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a decision tree classifier object \n",
    "tree_clf = DecisionTreeClassifier(criterion='entropy', random_state=42) \n",
    "# Fit the classifier to the training data \n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "We can evaluate the performance of the model on the test set using scikit-learn's *accuracy_score* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set \n",
    "y_pred = tree_clf.predict(X_test) \n",
    "# Calculate the accuracy of the model \n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Model\n",
    "\n",
    "We can visualize the decision tree using scikit-learn's *plot_tree* function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree \n",
    "plot_tree(tree_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import with pip the package <a href=\"https://pypi.org/project/dtreeviz/\">dtreeviz</a> to visualise nicely the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = dtreeviz(tree_clf, \n",
    "               X_train,\n",
    "               y_train,\n",
    "               feature_names=iris.feature_names, \n",
    "               class_names=[\"setosa\", \"versicolor\", \"virginica\"],\n",
    "               scale=2,\n",
    "               orientation='LR'\n",
    "\n",
    "               )\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Model\n",
    "\n",
    "We can tune the hyperparameters of the decision tree model to improve its performance. \n",
    "\n",
    "One important hyperparameter is the maximum depth of the tree. \n",
    "\n",
    "We can use scikit-learn's GridSearchCV function to search over different values of the maximum depth and find the best one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters to search over \n",
    "param_grid = {\"max_depth\": [1, 2, 3, 4, 5, 6, 7]} \n",
    "# Create a grid search object \n",
    "grid_search = GridSearchCV(tree_clf, param_grid, cv=5) \n",
    "# Fit the grid search object to the training data \n",
    "grid_search.fit(X_train, y_train) \n",
    "# Print the best hyperparameters found by the grid search \n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new decision tree classifier object with the best hyperparameters and fit it to the training data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new decision tree classifier object with the best hyperparameters \n",
    "tree_clf_tuned = DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=42) \n",
    "# Fit the classifier to the training data \n",
    "tree_clf_tuned.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree \n",
    "plot_tree(tree_clf_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='brown' size = '14pt' face=\"Century Schoolbook\"><b>Feature selection</b></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice prior to constructing the decision tree, it may be beneficial to perform dimensionality reduction techniques such as a Feature selection. This can enhance the likelihood of the decision tree to identify discriminative features.\n",
    "\n",
    "One way to do this is to examine the correlation between the variables in our dataset by plotting the Pearson Correlation among all attributes. We will use the full, clean dataset for this with the labels encoded. \n",
    "\n",
    "Recall: The *Pearson correlation coefficient (r)* is the most common way of measuring a linear correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_iris_set = X.copy()\n",
    "df_clean_iris_set['iris_name']=y\n",
    "df_clean_iris_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(df_clean_iris_set.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The initial observation provided by this heatmap is very valuable as it allows for a quick understanding of the predictive power of each feature.\n",
    "\n",
    "It is evident from the heatmap that the *petal length* and *petal width* exhibit the strongest correlations (in absolute terms) with the target classes, with respective values of 0.95 and 0.96.\n",
    "\n",
    "However, it should be noted that these two features also have a very high correlation with each other (0.96, the highest in the dataset), implying that they may be conveying the same information. Consequently, utilizing both of these features as inputs for the same model might not be advisable. \n",
    "\n",
    "If we look up to the example we can see that the column 2 from X (*petal length*) is in the root. Therefore, further exploration and comparison of these features is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "7. Drop the *petal width* column from the database and create a decision tree in a similar way with the example.\n",
    "\n",
    "8. Find the proper depth and evaluate the score for the decision tree model that you build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
